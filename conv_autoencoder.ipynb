{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('running on the GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('running on the CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.CIFAR10(root='ciphar10', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='ciphar10', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders\n",
    "\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "    \n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.squeeze(images[3])\n",
    "channels = ['red channel', 'green channel', 'blue channel']\n",
    "\n",
    "fig = plt.figure(figsize = (36, 36)) \n",
    "for idx in np.arange(rgb_img.shape[0]):\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    img = rgb_img[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(channels[idx])\n",
    "    width, height = img.shape\n",
    "    thresh = img.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(img[x][y], 2) if img[x][y] != 0 else 0\n",
    "            ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', size=8,\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 3 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "\n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoEncoder(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvAutoEncoder().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]/home/tuan/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 2500/2500 [00:33<00:00, 75.46it/s]\n",
      "  0%|          | 8/2500 [00:00<00:32, 75.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 11.647786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:36<00:00, 68.35it/s]\n",
      "  0%|          | 7/2500 [00:00<00:38, 64.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 11.384640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:35<00:00, 70.58it/s]\n",
      "  0%|          | 7/2500 [00:00<00:39, 63.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 11.361483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:37<00:00, 66.04it/s]\n",
      "  0%|          | 7/2500 [00:00<00:35, 69.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 11.345895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:43<00:00, 57.50it/s]\n",
      "  0%|          | 7/2500 [00:00<00:36, 67.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 11.332437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:37<00:00, 67.54it/s]\n",
      "  0%|          | 7/2500 [00:00<00:38, 65.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 11.319461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:44<00:00, 55.98it/s]\n",
      "  0%|          | 4/2500 [00:00<01:04, 38.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 11.309466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:45<00:00, 54.69it/s]\n",
      "  0%|          | 6/2500 [00:00<00:47, 52.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 11.303421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:39<00:00, 62.89it/s]\n",
      "  0%|          | 8/2500 [00:00<00:33, 73.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 11.299908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:45<00:00, 55.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 11.297476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in tqdm(train_loader):\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten images\n",
    "        images, _ = data\n",
    "#         images = images.view(images.size(0), -1)\n",
    "        images = images.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "\n",
    "# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 3, 32, 32)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# # plot the first ten input images and then reconstructed images\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
    "\n",
    "# # input images on top row, reconstructions on bottom\n",
    "# for images, row in zip([images, output], axes):\n",
    "#     for img, ax in zip(images, row):\n",
    "#         ax.imshow(np.squeeze(img))\n",
    "#         ax.get_xaxis().set_visible(False)\n",
    "#         ax.get_yaxis().set_visible(False)\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(output[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "    \n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
